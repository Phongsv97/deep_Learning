{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phong/work_space/deep_Learning/code/data_test\n",
      "/home/phong/work_space/deep_Learning/code/data_train\n"
     ]
    }
   ],
   "source": [
    "test_path = '/home/phong/work_space/deep_Learning/code/data_test'\n",
    "train_path = '/home/phong/work_space/deep_Learning/code/data_train'\n",
    "\n",
    "print(test_path)\n",
    "print(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2405 images belonging to 5 classes.\n",
      "Found 923 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Phac hoa images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 6s 24ms/step - loss: 0.7151 - accuracy: 0.9198 - val_loss: 0.2759 - val_accuracy: 0.8992\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 4s 19ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9014\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 4s 19ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9133\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 4s 18ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2547 - val_accuracy: 0.9166\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 5s 20ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2436 - val_accuracy: 0.9209\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 5s 20ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.9220\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 4s 18ms/step - loss: 9.5151e-04 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9242\n",
      "loss of 0.0006160453776828945; accuracy of 100.0%\n",
      "{'loss': [0.7151201367378235, 0.006137699820101261, 0.0026703402400016785, 0.0016404888592660427, 0.001238098251633346, 0.0010776343988254666, 0.0009515078272670507], 'accuracy': [0.9197505116462708, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.27594050765037537, 0.2688722312450409, 0.25422781705856323, 0.2546551525592804, 0.24358870089054108, 0.2478526085615158, 0.24571435153484344], 'val_accuracy': [0.8992416262626648, 0.9014084339141846, 0.9133260846138, 0.9165763854980469, 0.9209100604057312, 0.9219934940338135, 0.924160361289978], 'lr': [0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005]}\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#input shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "#\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "#flatten \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(5,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save\n",
    "model.save('model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-185f6ef32b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#pridiction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"model_dataflair3.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_batches' is not defined"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(test_batches)\n",
    "\n",
    "#pridiction\n",
    "model = keras.models.load_model(r\"model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "\n",
    "word_dict = {0:'One',1:'Two',2:'Five',3:'Ok',4:'Palm'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
