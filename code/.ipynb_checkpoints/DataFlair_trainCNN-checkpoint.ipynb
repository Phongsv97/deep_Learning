{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phong/work_space/deep_Learning/code/data_test\n",
      "/home/phong/work_space/deep_Learning/code/data_train\n"
     ]
    }
   ],
   "source": [
    "test_path = '/home/phong/work_space/deep_Learning/code/data_test'\n",
    "train_path = '/home/phong/work_space/deep_Learning/code/data_train'\n",
    "\n",
    "print(test_path)\n",
    "print(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2405 images belonging to 5 classes.\n",
      "Found 923 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Phac hoa images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[10,7] labels_size=[10,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-8-8a441b7d2f61>:36) ]] [Op:__inference_train_function_1571]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a441b7d2f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For getting next batch of imgs...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/phonglt/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[10,7] labels_size=[10,5]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-8-8a441b7d2f61>:36) ]] [Op:__inference_train_function_1571]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#input shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "#\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "#flatten \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(5,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save\n",
    "model.save('model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.22303485870361328; accuracy of 89.99999761581421%\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 413,959\n",
      "Trainable params: 413,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1d30681710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "Ok   Three   Three   Two   Three   Three   Ok   One   Five   Five   "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "Ok   Three   Three   One   Three   Three   Ok   One   Five   Five   (10, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8353376984596252, 0.03106500580906868, 0.006487695500254631],\n",
       " 'accuracy': [0.8611034154891968, 0.9941779971122742, 0.9994454979896545],\n",
       " 'val_loss': [0.553476870059967, 0.6327352523803711, 0.6370905637741089],\n",
       " 'val_accuracy': [0.8439344167709351, 0.8537704944610596, 0.8459016680717468],\n",
       " 'lr': [0.001, 0.001, 0.0005]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches)\n",
    "\n",
    "#pridiction\n",
    "model = keras.models.load_model(r\"model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "\n",
    "word_dict = {0:'One',1:'Two',2:'Three',3:'Four',4:'Five',5:'Ok',6:'Palm'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
